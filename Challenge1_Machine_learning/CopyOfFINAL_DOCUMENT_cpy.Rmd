---
title: "Challenge 1"
author: "Mitchell Ardolf, Evan Weibe, Adam Vazquez Rosales"
date: "10/9/2023"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)


# package requirements
library(dslabs)
library(readr)
library(dplyr)
library(tidyverse)
library(tidymodels)
library(viridis)
library(glmnet)
library(caret)
library(ggrepel)
tidymodels_prefer(quiet=TRUE)


# Given fuctions
mnist <- read_mnist("~/Mscs 341 F23/Class/Data")

# Plots a digit as an image
plot_digit <- function(image) {
  a_digit <- matrix(image, nrow=28)
  image(a_digit[,28:1])
}

# This function returns a matrix with all the images corresponding to a digit (0-9) 
# The matrix will have 784 columns and the number of rows corresponds to the
# number of images

#This function gets a matrix of images from the training data set 
get_image_digit_train <- function (digit) {
  idx <- mnist$train$labels==digit
  images <- mnist$train$images[idx,]
  return (images)
}


#This function gets a matrix of images from the testing data set 
get_image_digit_test <- function (digit) {
  idx <- mnist$test$labels==digit
  images <- mnist$test$images[idx,]
  return (images)
}

#contrast digit takes a digit and contrasts it such that anything darker than 128 is now 255 and anthing less is now 0 
contrast_digit <- function(image){
  dark_idx = (image>128)
  image[dark_idx]=255   # Turn dark pixel indexes to 255
  image[!dark_idx]=0    # Turn non-dark pixel indexes to 0 (we use symbol !)
  return (image)
}
```

# Feature Definition.

The first feature we plan to use is the width of the number. This is the distance betweeen the left border and the right border of the image. The borders are calculated by being the furthest out rows in our matrix where there is a sum of values greater than 0. We think that 4s will have a larger width than the 1s.

```{r Width Functions}

get_size <- function(images){
  
  n_images <- nrow(images)
  
  
  size_val <- vector(mode="integer", length=n_images)
  for (i in 1:n_images) {
    size_val[i] = get_width(images[i,])  
  }
  return (size_val)
}
```

The second feature we are using is the amount of dark pixels within the upper half. This is calculated my counting the number of dark pixels in the first half of the vector (1 to 392). 
We think that fours will have a larger number of dark pixels in the top half since they typically have 2 vertical lines in the top half, while 1s typically only have the one line in the upper half.

```{r Dark Pixels Top Half Functions}
count_dark_pixels <- function (image){ # This function counts the dark pixels in the loop 
  dark_idx <- image>128
  num_pixels <- sum(dark_idx)
  return(num_pixels)
}

get_num_pixels <- function(images){
  # We get all images corresponding to a digit

  n_images <- nrow(images)
  
  # We use a for loop to calculate the ink of each digit
  dark <- vector(mode="integer", length=n_images)
  for (i in 1:n_images) {
    dark[i] = count_dark_pixels(images[i, 1:392])
  }
  return (dark)
}
```


# Dataset Creation.

```{r Dataset Creation}
set.seed(123)

# This function gets the count of dark pixels for the top half of contrasted images.
# We are writing this now to have contrasted image values within the same table, even
# though the "Changing things up (I)" still has awhile to wait.
contrast <- function(images){
  # We get all images corresponding to a digit

  n_images <- nrow(images)
  
  dark <- vector(mode="integer", length=n_images)
  jeff<- vector(mode="integer", length=n_images)
  for (i in 1:n_images) {
    dark[i] = count_dark_pixels(contrast_digit(images[i, 1:392]))
  }
  return (dark)
}


# This function gets the width of contrasted images.
# We are writing this now to have contrasted image values within the same table, even
# though the "Changing things up (I)" still has awhile to wait.
get_size_light <- function(images){
  
  n_images <- nrow(images)
  
  size_val <- vector(mode="integer", length=n_images)
  for (i in 1:n_images) {
    size_val[i] = get_width(contrast_digit(images[i,])) #width of the image
  }
  return (size_val)
}

#Get all ones and fours for training.
image1_train <- get_image_digit_train(1)
image4_train <- get_image_digit_train(4)

# Get row numbers (indexes) for 1s and 4s in training. 
train_count_1_bool <- mnist$train$labels==1 # Get boolean of 1s in train.
length1_train <- sum(train_count_1_bool) # Get number of 1s for the loop.
idx1_train <- vector(mode = "integer", length = length1_train)
n=1
for (i in 1:60000){
  
  if (train_count_1_bool[i] == TRUE){
    if (idx1_train[n] != 0){
      n = n+1
    }
    idx1_train[n] <- i
    
  }
  
}

train_count_4_bool <- mnist$train$labels==4 # Get boolean of 4s in train.
length4_train <- sum(train_count_4_bool) # Get number of 4s for the loop.
idx4_train <- vector(mode = "integer", length = length4_train)
n=1
for (i in 1:60000){
  
  if (train_count_4_bool[i] == TRUE){
    if (idx4_train[n] != 0){
      n = n+1
    }
    idx4_train[n] <- i
    
  }
  
}


#Get all ones and fours for testing.
image1_test <- get_image_digit_test(1)
image4_test <- get_image_digit_test(4)

# Get row numbers (indexes) for 1s and 4s in training. 
test_count_1_bool <- mnist$test$labels==1 # Get boolean of 1s in test.
length1_test <- sum(test_count_1_bool) # Get number of 1s for the loop.
idx1_test <- vector(mode = "integer", length = length1_test)
n=1
for (i in 1:10000){
  
  if (test_count_1_bool[i] == TRUE){
    if (idx1_test[n] != 0){
      n = n+1
    }
    idx1_test[n] <- i
    
  }
  
}

test_count_4_bool <- mnist$test$labels==4 # Get boolean of 4s in test.
length4_test <- sum(test_count_4_bool) # Get number of 4s for the loop.
idx4_test <- vector(mode = "integer", length = length4_test)
n=1
for (i in 1:10000){
  
  if (test_count_4_bool[i] == TRUE){
    if (idx4_test[n] != 0){
      n = n+1
    }
    idx4_test[n] <- i
    
  }
  
}


# Calculate features for training get the number of pixels for ones.
image_1_dark_train <- get_num_pixels(image1_train) # Dark pixels top half.
# Dark pixels top half on the contrasted digit.
image_1_dark_cont_train <- contrast(image1_train) 

#Calculate features for training get width of the image for ones.
image_1_size_train <- get_size(image1_train)# Width for ones.
# Width for contrasted ones.
image_1_size_cont_train <- get_size_light(image1_train)

#Calculate features for testing get the number of pixels for ones. 
image_1_dark_test <- get_num_pixels(image1_test)# Dark pixels top half.
# Dark pixels top half on the contrasted digit.
image_1_dark_cont_test <- contrast(image1_test)

#Calculate features for testing get size of the image for ones.
image_1_size_test <- get_size(image1_test)# Width for ones.
# Width for contrasted ones.
image_1_size_cont_test <- get_size_light(image1_test)


#Combine the tables to get our training and testing tables.
combined_table_1_train <- data.frame(Size = image_1_size_train, Size_Contrast = image_1_size_cont_train, Dark = image_1_dark_train, Dark_Contrast = image_1_dark_cont_train,   Number = 1, idx = idx1_train)

combined_table_1_test <- data.frame(Size = image_1_size_test, Size_Contrast = image_1_size_cont_test, Dark = image_1_dark_test, Dark_Contrast = image_1_dark_cont_test, Number = 1, idx = idx1_test)


#Calculate features for training get the number of pixels for fours.
image_4_dark_train <- get_num_pixels(image4_train)# Dark pixels top half.
# Dark pixels top half on the contrasted digit.
image_4_dark_cont_train <- contrast(image4_train)

#Calculate features for training get size of the image for fours. 
image_4_size_train <- get_size(image4_train)# Width for fours.
# Width for contrasted fours.
image_4_size_cont_train <- get_size_light(image4_train)

#Calculate features for testing get the number of pixels for fours.
image_4_dark_test <- get_num_pixels(image4_test)# Dark pixels top half.
# Dark pixels top half on the contrasted digit.
image_4_dark_cont_test <- contrast(image4_test)

#Calculate features for testing get size of the image for fours.
image_4_size_test <- get_size(image4_test)# Width for fours.
# Width for contrasted fours.
image_4_size_cont_test <- get_size_light(image4_test)

#Combine the tables to get our training and testing tables.
combined_table_4_train <- data.frame(Size = image_4_size_train, Size_Contrast = image_4_size_cont_train,  Dark = image_4_dark_train, Dark_Contrast = image_4_dark_cont_train,  Number = 4, idx = idx4_train)

combined_table_4_test <- data.frame(Size = image_4_size_test, Size_Contrast = image_4_size_cont_test, Dark = image_4_dark_test, Dark_Contrast = image_4_dark_cont_test, Number = 4, idx =idx4_test)

# Combine the 1s and 4s.
combined_train <- bind_rows(combined_table_1_train, combined_table_4_train)
combined_test <- bind_rows(combined_table_1_test, combined_table_4_test)

# Sample testing and training.
set.seed(123)
sampled_data_train <- combined_train %>% sample_n(800, replace = FALSE)
sampled_data_test <- combined_test %>% sample_n(200, replace = FALSE)

# Create File Paths for CSVs.
file_path_test <- "~/Mscs 341 F23/Project/Mitch, Evan, Adam/Data/sampled_data_test.csv"
file_path_train <- "~/Mscs 341 F23/Project/Mitch, Evan, Adam/Data/sampled_data_train.csv"

# Write the files.
write.csv(sampled_data_train, file = file_path_train)
write.csv(sampled_data_test, file = file_path_test)

```

# Model Creation.

```{r}
# Data for the models.

# These renames accommodate for the model building, which was initially written 
# in different files, which pulled the CSVs, then renamed the testing data to 
# raw_test and the training data to raw_train.

# Testing data set. 
raw_test <- sampled_data_test

# Training data set.
raw_train <- sampled_data_train
```

### KNN3.

```{r}
set.seed(129)
# Data manipulation.
# We need the output as a factor for classification.
test_tbl <- raw_test %>%
  mutate(Number = as.factor(Number))
  
train_tbl <- raw_train %>%
  mutate(Number = as.factor(Number))

# Model Building.

# Calculate error rate with given k.
calc_error4 <- function(kNear, train_tbl, test_tbl) {
  
  knn_model <- knn3(Number ~ Size + Dark, data = train_tbl, k = kNear)
  pred <- predict(knn_model, test_tbl, type="class") 
  
  mean (pred!=test_tbl$Number)
}

# Test many values of k to optimize the model.
error_test <- vector(length = 100) 
for(i in 1:100){
  error_test[i] <- calc_error4(i, train_tbl, test_tbl)
}
error_tbl <- tibble (k=1:100,
                     accuracy = error_test)

# Plot the error as k varies.
error_tbl %>%
  ggplot(aes(x=k, y=accuracy)) +
  geom_line()

# Find where we minimize the error rate (maximize the accuracy).
min_k_tbl <- error_tbl %>% 
  slice_max(accuracy) %>%
  arrange(desc(k)) %>%
  slice(1)
  
min_k_tbl

# Set our recipe.
NumberRecipe <- 
  recipe(Number ~ Size + Dark, data=train_tbl)

# Set our model type.
Knn_model <- nearest_neighbor(neighbors = min_k_tbl$k) %>%
  set_engine("kknn") %>%
  set_mode("classification")

# Set our work flow.
Knn_wflow <- workflow() %>%
  add_recipe(NumberRecipe) %>% # Mix the recipe and model.
  add_model(Knn_model) 

# Fit the model.
Knn_fit_digit <- fit(Knn_wflow, train_tbl)

# Table to evaluate our model.
augmented_test <-augment(Knn_fit_digit, test_tbl)

# Accuracy.
augmented_test%>%
  accuracy(Number, .pred_class)

```


### Logistic regression.

```{r}
# Data manipulation.
# We need the output as a factor for classification.
test_tbl_logit <- raw_test %>%
  mutate(Number = as.factor(Number))
  
train_tbl_logit <- raw_train %>%
  mutate(Number = as.factor(Number))

# Model building.

# Set our recipe.
NumberRecipe_logit <- 
  recipe(Number ~ Size + Dark, data=train_tbl_logit)

# Set our model type.
logit_model <- logistic_reg() %>%
  set_engine("glm") %>% # Decide engine.
  set_mode("classification") # We are wanting to use classification here.

# Work flow.
logit_wflow <- workflow() %>%
  add_recipe(NumberRecipe_logit) %>% # Mix recipe and model.
  add_model(logit_model) 

# Fit the model on our training data set.
logit_fit_digit <- fit(logit_wflow, train_tbl_logit)

# Table to evaluate our model on our testing data set.
augmented_test_logit <- augment(logit_fit_digit, test_tbl)

# Accuracy
augmented_test_logit%>%
  accuracy(Number, .pred_class)

```

## Evaluate the Chosen Model.

We choose the logistic regression model for our since the accuracy of the logistic is
greater than the accuracy of the KNN model. Therefore the misclassification rate in the
logsitic regression model is less than the misclassification rate in the KNN model.

```{r}
# Accuracy, Logistic Regression.
acc_log<-augmented_test_logit%>%
  accuracy(Number, .pred_class)

# Accuracy, KNN.
acc_knn<-augmented_test%>%
  accuracy(Number, .pred_class)

# Calculate Misclassification Rate.
mcr_log <- 1 - acc_log$.estimate
mcr_knn <- 1 - acc_knn$.estimate

mcr_df = data.frame(knn = mcr_knn, logistic.regression = mcr_log)

mcr_df
```

Below is the confusion matrix for the Logistic Regression Model.

```{r}
augmented_test_logit%>%
  conf_mat(Number, .pred_class)
```

# Visualization.

## Probabilities and the Decision Boundary

```{r}
# Decision boundary

#Find range of size.
range(augmented_test_logit$Size)
# gridvec1 as range of size (unit of 1 increase).
gridvec1 <- c(4:20)

#Find range of dark pixels top half.
range(augmented_test_logit$Dark)
# gridvec2 as range of size (unit of 1 increase).
gridvec2 <- c(13:81)

# Grid of theoretical values is created by every possible combination
# of our 2 ranges.
(grid_tbl <- expand_grid(Size=gridvec1, Dark=gridvec2))

# Get the graph of our probabilities and the decision boundary.
augment(logit_fit_digit, grid_tbl)%>% # Percentages
ggplot(mapping = aes(x = Size, y = Dark, z = .pred_4, fill = .pred_4)) +
  geom_raster() + # Assigns color to each square of the grid.
  stat_contour(breaks=c(0.5), color="darkolivegreen1") + #Bayes/Decision boundary
  scale_fill_viridis(option="magma") 

```

## Misclassified Digits

# Changing things up (I)

```{r}
# set our recipe.
NumberRecipe_contrast <- 
  recipe(Number ~ Size_Contrast + Dark, data=train_tbl)

# set our model type.
logit_model_contrast <- logistic_reg() %>%
  set_engine("glm") %>% # decide engine
  set_mode("classification") # we are so wanting to do classification here

# work flow.
logit_wflow_contrast <- workflow() %>%
  add_recipe(NumberRecipe_contrast) %>% # mix recipe and model
  add_model(logit_model_contrast) 

# fit the model.
logit_fit_contrast <- fit(logit_wflow_contrast, train_tbl)

```




We now need to test our model.
```{r}
# table to evaluate our table
augmented_test <-augment(logit_fit_contrast, test_tbl)

#confusion matrix
augment(logit_fit_contrast, test_tbl)%>%
  conf_mat(Number, .pred_class)

# accuracy
ctu1_acc<-augmented_test%>%
  accuracy(Number, .pred_class)

# Misclassification Rate
ctu1_mcr <- 1 - ctu1_acc
ctu1_mcr

```



# Changing things up (II)

## Pulling in our sixes and feature calculation.

```{r Pulling in 6s}
image6_train <- get_image_digit_train(6)

train_count_6_bool <- mnist$train$labels==6 # get boolean of 1s in train
length6_train <- sum(train_count_6_bool) # get number of 1s for the loop
idx6_train <- vector(mode = "integer", length = length6_train)
n=1
for (i in 1:60000){
  
  if (train_count_6_bool[i] == TRUE){
    if (idx6_train[n] != 0){
      n = n+1
    }
    idx6_train[n] <- i
    
  }
  
}

image6_test <- get_image_digit_test(6)

test_count_6_bool <- mnist$test$labels==6 # get boolean of 1s in test
length6_test <- sum(test_count_6_bool) # get number of 1s for the loop
idx6_test <- vector(mode = "integer", length = length6_test)
n=1
for (i in 1:10000){
  
  if (test_count_6_bool[i] == TRUE){
    if (idx6_test[n] != 0){
      n = n+1
    }
    idx6_test[n] <- i
    
  }
  
}

#Calculate features for training get the number of pixels for sixes.
image_6_dark_train <- get_num_pixels(image6_train)# Dark pixels top half.
# Dark pixels top half on the contrasted digit.
image_6_dark_cont_train <- contrast(image6_train)

#Calculate features for training get size of the image for sixes. 
image_6_size_train <- get_size(image6_train)# Width for sixes.
# Width for contrasted sixes.
image_6_size_cont_train <- get_size_light(image6_train)

#Calculate features for testing get the number of pixels for sixes. 
image_6_dark_test <- get_num_pixels(image6_test)# Dark pixels top half.
# Dark pixels top half on the contrasted digit.
image_6_dark_cont_test <- contrast(image6_test)

#calculate features for testing get size of the image for sixes. 
image_6_size_test <- get_size(image6_test)# Width for sixes.
# Width for contrasted sixes.
image_6_size_cont_test <- get_size_light(image6_test)

#Combine the tables to get our training and testing tables 
combined_table_6_train <- data.frame(Size = image_6_size_train, Size_Contrast = image_6_size_cont_train,  Dark = image_6_dark_train, Dark_Contrast = image_6_dark_cont_train,  Number = 6, idx = idx6_train)

combined_table_6_test <- data.frame(Size = image_6_size_test, Size_Contrast = image_6_size_cont_test, Dark = image_6_dark_test, Dark_Contrast = image_6_dark_cont_test, Number = 6, idx =idx6_test)

# Sample testing and training.
set.seed(123)
sampled_data_train6 <- combined_table_6_train %>% sample_n(400, replace = FALSE)
sampled_data_test6 <- combined_table_6_test %>% sample_n(100, replace = FALSE)

# Add to datasets with 1s and 4s.
total_sample_with6_train <- bind_rows(sampled_data_train, sampled_data_train6)
total_sample_with6_test <- bind_rows(sampled_data_test, sampled_data_test6)
```

## Build our model

Since we are adding a third possible outcome, we will need to use multinomial logistic regression instead of regular logistic regression.

```{r}
# Get Number (our output as factor)
total_sample_with6_train <- total_sample_with6_train %>%
  mutate(Number = as.factor(Number))%>%
mutate(Number=fct_relevel(Number, c("1","4","6")))

total_sample_with6_test <- total_sample_with6_test %>%
  mutate(Number = as.factor(Number))%>%
mutate(Number=fct_relevel(Number, c("1","4","6")))

# Model building.

# Set our recipe.

digit_recipe_multi <- recipe(Number ~ Size + Dark, data=total_sample_with6_train)

# Set our model type.

logit_model_multi <- multinom_reg(
  mode = "classification",
  engine = "nnet")

# Work flow.

digit_wflow_multi <- workflow() %>%
  add_recipe(digit_recipe_multi) %>%
  add_model(logit_model_multi) 

# Fit the model on our training data set.

digit_fit_multi <- fit(digit_wflow_multi, total_sample_with6_train)

# Table to evaluate our model on our testing data set.

augment_test_multi <- augment(digit_fit_multi, total_sample_with6_test)

```

## Confusion Matrix and Accuracy

```{r}
# Accuracy
acc_multi<-augment_test_multi%>%
  accuracy(Number, .pred_class)
acc_multi

# Confusion Matrix
augment_test_multi %>%
  conf_mat(Number, .pred_class)
```

## Decision Boundary

```{r}
augment(digit_fit_multi, grid_tbl)%>% # Percentages, 
  #grid_tbl was defined for the other decision boundary and before.
ggplot(mapping = aes(x = Size, y = Dark, z = .pred_4 , color = .pred_class)) +
  geom_point + # Assigns color to each square of the grid.
  scale_fill_viridis( option="inferno") +
  stat_contour(mapping = aes(x = Size, y = Dark, z = .pred_4), 
               breaks=c(0.5), 
               color="darkolivegreen1") + #Bayes/Decision boundary
  stat_contour(mapping = aes(x = Size, y = Dark, z = .pred_1),
               breaks=c(0.5), 
               color="darkolivegreen1") +  #Bayes/Decision boundary
  annotate(geom="text", 
           x=17, 
           y=20, 
           label="Pred 4",
           color="darkolivegreen1") +
  annotate(geom="text",
           x=14, 
           y=60, 
           label="Pred 6",
           color="darkolivegreen1")+
  annotate(geom="text",
           x= 6, 
           y=50, 
           label="Pred 1",
           color="darkolivegreen1")

  
```

